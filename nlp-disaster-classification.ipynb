{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport string\nimport spacy\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nK = keras.backend\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Flatten, Dropout, Embedding\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-10T12:55:56.807545Z","iopub.execute_input":"2021-06-10T12:55:56.808027Z","iopub.status.idle":"2021-06-10T12:56:06.364676Z","shell.execute_reply.started":"2021-06-10T12:55:56.807908Z","shell.execute_reply":"2021-06-10T12:56:06.362317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and review data","metadata":{}},{"cell_type":"code","source":"# loading train and test set from kqggle public datasets\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\n\ntrain.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:56:06.366692Z","iopub.execute_input":"2021-06-10T12:56:06.367213Z","iopub.status.idle":"2021-06-10T12:56:06.465782Z","shell.execute_reply.started":"2021-06-10T12:56:06.367162Z","shell.execute_reply":"2021-06-10T12:56:06.464813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# common abbreviations and contractions\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"â‚¬\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bro\" : \"brother\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"tv\": \"television\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w/\" : \"with\",\n    \"w/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\",\n    \"aren't\" : \"are not\",\n    \"can't\" : \"cannot\",\n    \"couldn't\" : \"could not\",\n    \"couldnt\" : \"could not\",\n    \"didn't\" : \"did not\",\n    \"doesn't\" : \"does not\",\n    \"doesnt\" : \"does not\",\n    \"don't\" : \"do not\",\n    \"hadn't\" : \"had not\",\n    \"hasn't\" : \"has not\",\n    \"haven't\" : \"have not\",\n    \"havent\" : \"have not\",\n    \"he'd\" : \"he would\",\n    \"he'll\" : \"he will\",\n    \"he's\" : \"he is\",\n    \"i'd\" : \"I would\",\n    \"i'd\" : \"I had\",\n    \"i'll\" : \"I will\",\n    \"i'm\" : \"I am\",\n    \"im\": 'I am',\n    \"isn't\" : \"is not\",\n    \"it's\" : \"it is\",\n    \"it'll\":\"it will\",\n    \"i've\" : \"I have\",\n    \"let's\" : \"let us\",\n    \"mightn't\" : \"might not\",\n    \"mustn't\" : \"must not\",\n    \"shan't\" : \"shall not\",\n    \"she'd\" : \"she would\",\n    \"she'll\" : \"she will\",\n    \"she's\" : \"she is\",\n    \"shouldn't\" : \"should not\",\n    \"shouldnt\" : \"should not\",\n    \"that's\" : \"that is\",\n    \"thats\" : \"that is\",\n    \"there's\" : \"there is\",\n    \"theres\" : \"there is\",\n    \"they'd\" : \"they would\",\n    \"they'll\" : \"they will\",\n    \"they're\" : \"they are\",\n    \"theyre\":  \"they are\",\n    \"they've\" : \"they have\",\n    \"we'd\" : \"we would\",\n    \"we're\" : \"we are\",\n    \"weren't\" : \"were not\",\n    \"we've\" : \"we have\",\n    \"what'll\" : \"what will\",\n    \"what're\" : \"what are\",\n    \"what's\" : \"what is\",\n    \"what've\" : \"what have\",\n    \"where's\" : \"where is\",\n    \"who'd\" : \"who would\",\n    \"who'll\" : \"who will\",\n    \"who're\" : \"who are\",\n    \"who's\" : \"who is\",\n    \"who've\" : \"who have\",\n    \"won't\" : \"will not\",\n    \"wouldn't\" : \"would not\",\n    \"you'd\" : \"you would\",\n    \"you'll\" : \"you will\",\n    \"you're\" : \"you are\",\n    \"you've\" : \"you have\",\n    \"'re\": \" are\",\n    \"wasn't\": \"was not\",\n    \"we'll\":\" will\",\n    \"didn't\": \"did not\"\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:56:06.469209Z","iopub.execute_input":"2021-06-10T12:56:06.469545Z","iopub.status.idle":"2021-06-10T12:56:06.504465Z","shell.execute_reply.started":"2021-06-10T12:56:06.469519Z","shell.execute_reply":"2021-06-10T12:56:06.503095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:56:06.506876Z","iopub.execute_input":"2021-06-10T12:56:06.507613Z","iopub.status.idle":"2021-06-10T12:56:06.534646Z","shell.execute_reply.started":"2021-06-10T12:56:06.507571Z","shell.execute_reply":"2021-06-10T12:56:06.533205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop duplicates\ntrain.drop_duplicates(subset=['text'], inplace=True)\n\n# get the features we want to analyze\nX_train = train['text'] \n# the labels\ny_train = train['target'] \n\nX_test = test['text'] ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:56:06.536433Z","iopub.execute_input":"2021-06-10T12:56:06.536922Z","iopub.status.idle":"2021-06-10T12:56:06.554051Z","shell.execute_reply.started":"2021-06-10T12:56:06.536878Z","shell.execute_reply":"2021-06-10T12:56:06.552848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Create our list of punctuation marks\npunctuations = string.punctuation\npunctuations += '...'\npunctuations += '....'\n\n# Create list of stopwords\nnlp = spacy.load('en')\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nparser = English()\n\n# Preliminary text cleaning\nclass Cleaner(TransformerMixin):\n    \n    def transform(self, X, **transform_params):\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\ndef clean_text(text):\n    # split data entry\n    text = text.split()\n    # omit hashtags\n    text = [word for word in text if not word.startswith('#')]\n    # join data entry back for tokenization and lemmatization\n    text = ' '.join(text)\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    mytokens = parser(text)\n    # Lemmatizing each token and converting each token into lowercase\n    mytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens]\n    # remove non-ascii\n    mytokens_enc = [word.encode(\"ascii\", \"ignore\") for word in mytokens]\n    mytokens = [word.decode() for word in mytokens_enc]\n    # Remove stop words\n    mytokens = [word for word in mytokens if word not in stop_words and word not in punctuations]\n    # replace numbers with 'number' token\n    mytokens = [word if not word.isnumeric() else 'number' for word in mytokens]\n    # replace abbreviations\n    mytokens = [abbreviations[word] if word in abbreviations.keys() else word for word in mytokens]\n    # remove mentions\n    mytokens = [word if not word.startswith('@') else 'user' for word in mytokens]\n    # removing URLs\n    mytokens = [word if not word.startswith('http') else 'url' for word in mytokens]\n    # remove other encodings\n    mytokens = [word for word in mytokens if not word.__contains__('\\x89')]\n    # remove out-of-vocabulary symbols\n    mytokens = [word for word in mytokens if not word.__contains__('amp')]\n    # strip any '/' character \n    mytokens = [word.replace('/', '') for word in mytokens]\n    # remove 2-letter words\n    mytokens = [word for word in mytokens if len(word) > 2]\n    # remove non-alphabetic words\n    mytokens = [word for word in mytokens if word.isalpha()]\n    return mytokens","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:56:06.555648Z","iopub.execute_input":"2021-06-10T12:56:06.556087Z","iopub.status.idle":"2021-06-10T12:56:08.239971Z","shell.execute_reply.started":"2021-06-10T12:56:06.556046Z","shell.execute_reply":"2021-06-10T12:56:08.238842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vocabulary reduction","metadata":{}},{"cell_type":"code","source":"# get train data vocabulary\ndef get_vocab(data):\n    # instantiate Counter()\n    vocab = Counter()\n    # preprocess data\n    cleaned = Cleaner().fit_transform(data)\n    # count number of each word occurrences in data \n    for line in cleaned:\n        vocab.update(line)\n    return vocab\n\n# keep tokens with a min occurrence\ndef reduce_vocab(min_occurane):\n    tokens = [k for k, c in vocab.items() if c >= min_occurane]\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:56:08.243108Z","iopub.execute_input":"2021-06-10T12:56:08.243822Z","iopub.status.idle":"2021-06-10T12:56:08.251681Z","shell.execute_reply.started":"2021-06-10T12:56:08.243774Z","shell.execute_reply":"2021-06-10T12:56:08.250234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final cleaning of data: omit words that are not present in reduced vocabulary\ndef reduced_vocab_data(data, vocab):\n    reduced_data = []\n    # walk through preprocessed data\n    for line in data:\n        # get only those words that are in reduced vocabulary\n        tokens = [word for word in line if word in vocab]\n        # update list with cleaned line of data\n        reduced_data.append(' '.join(tokens))\n    return reduced_data","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:56:08.255614Z","iopub.execute_input":"2021-06-10T12:56:08.255913Z","iopub.status.idle":"2021-06-10T12:56:08.266472Z","shell.execute_reply.started":"2021-06-10T12:56:08.255884Z","shell.execute_reply":"2021-06-10T12:56:08.265323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Classifiers","metadata":{}},{"cell_type":"code","source":"X_t, X_v, y_t, y_v = train_test_split(X_train, y_train, test_size=0.15, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:59:20.643107Z","iopub.execute_input":"2021-06-10T12:59:20.643535Z","iopub.status.idle":"2021-06-10T12:59:20.654622Z","shell.execute_reply.started":"2021-06-10T12:59:20.643507Z","shell.execute_reply":"2021-06-10T12:59:20.652042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifiers = [('lr', LogisticRegression()),\n              ('svc', SVC()),\n              ('gbc', GradientBoostingClassifier(n_estimators=200))]\n\ntfidf_vector = TfidfVectorizer(tokenizer=clean_text)\n\ndef get_pipe(classifiers):\n    pipes = []\n    for classifier in classifiers:\n        # Create pipeline using Bag of Words\n        pipes.append((classifier[0], Pipeline([('vectorizer', tfidf_vector), classifier])))\n    return pipes\n\npipes = get_pipe(classifiers)\n\nfor pipe in pipes:\n    # model generation\n    pipe[1].fit(X_t, y_t)\n\n    # Predicting with a test dataset\n    predicted = pipe[1].predict(X_v)\n\n    # Model Accuracy\n    print(pipe[0] + ':', metrics.accuracy_score(y_v, predicted))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:16:50.117051Z","iopub.execute_input":"2021-06-05T19:16:50.117413Z","iopub.status.idle":"2021-06-05T19:17:02.702957Z","shell.execute_reply.started":"2021-06-05T19:16:50.117381Z","shell.execute_reply":"2021-06-05T19:17:02.702055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get a stacking ensemble of models\ndef get_stacking(pipes):\n    # define the base models\n    level0 = list()\n    for pipe in pipes:\n        level0.append((str(pipe[0]), pipe[1]))\n    # define meta learner model\n    level1 = LogisticRegression()\n    # define the stacking ensemble\n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=3)\n    return model\n\nstacking = get_stacking(pipes)\nstacking.fit(X_t, y_t)\n# Predicting with a test dataset\npredicted = stacking.predict(X_v)\n# Model Accuracy\nprint('stacking:', metrics.accuracy_score(y_v, predicted))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T18:33:30.513217Z","iopub.execute_input":"2021-06-05T18:33:30.51356Z","iopub.status.idle":"2021-06-05T18:34:06.625711Z","shell.execute_reply.started":"2021-06-05T18:33:30.513529Z","shell.execute_reply":"2021-06-05T18:34:06.624809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest[\"target\"] = stacking.predict(X_test)\nsubmission = test[[\"id\", \"target\"]]\nsubmission.to_csv(\"sub_st.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multichannel Convolutional Neural Network","metadata":{}},{"cell_type":"markdown","source":"## Tokenization & Encoding","metadata":{}},{"cell_type":"code","source":"# fit a tokenizer\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\n# calculate the maximum document length\ndef max_length(lines):\n    return int(np.mean([len(line) for line in lines]))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:40:40.547118Z","iopub.execute_input":"2021-06-10T13:40:40.547539Z","iopub.status.idle":"2021-06-10T13:40:40.553725Z","shell.execute_reply.started":"2021-06-10T13:40:40.547492Z","shell.execute_reply":"2021-06-10T13:40:40.552128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encode data\ndef encode_data(tokenizer, lines, size):\n    # integer encode\n    encoded = tokenizer.texts_to_sequences(lines)\n    # pad encoded sequences\n    padded = pad_sequences(encoded, maxlen=size, padding='post')\n    return padded","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:40:42.059081Z","iopub.execute_input":"2021-06-10T13:40:42.059529Z","iopub.status.idle":"2021-06-10T13:40:42.065392Z","shell.execute_reply.started":"2021-06-10T13:40:42.059499Z","shell.execute_reply":"2021-06-10T13:40:42.063745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define model","metadata":{}},{"cell_type":"code","source":"# define model\ndef define_model(length, vocab_size):\n    # channel 1\n    input1 = Input(shape=(length,))\n    embedding1 = Embedding(vocab_size, 25)(input1)\n    conv1 = Conv1D(filters=8, kernel_size=2, activation='elu')(embedding1)\n    drop1 = Dropout(0.4)(conv1)\n    pool1 = MaxPooling1D(pool_size=2)(drop1)\n    flat1 = Flatten()(pool1)\n    # channel 2\n    input2 = Input(shape=(length,))\n    embedding2 = Embedding(vocab_size, 25)(input2)\n    conv2 = Conv1D(filters=8, kernel_size=3, activation='elu')(embedding2)\n    drop2 = Dropout(0.4)(conv2)\n    pool2 = MaxPooling1D(pool_size=2)(drop2)\n    flat2 = Flatten()(pool2)\n    # channel 3\n    input3 = Input(shape=(length,))\n    embedding3 = Embedding(vocab_size, 25)(input3)\n    conv3 = Conv1D(filters=8, kernel_size=4, activation='elu')(embedding3)\n    drop3 = Dropout(0.4)(conv3)\n    pool3 = MaxPooling1D(pool_size=2)(drop3)\n    flat3 = Flatten()(pool3)\n    # merge\n    merged = concatenate([flat1, flat2, flat3])\n    # interpretation\n    drop4 = Dropout(0.1)(merged)\n    dense2 = Dense(10, activation='elu')(drop4)\n    outputs = Dense(1, activation='sigmoid')(dense2)\n    model = Model(inputs=[input1, input2, input3], outputs=outputs)\n    # compile\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:40:45.002854Z","iopub.execute_input":"2021-06-10T13:40:45.003304Z","iopub.status.idle":"2021-06-10T13:40:45.016722Z","shell.execute_reply.started":"2021-06-10T13:40:45.00326Z","shell.execute_reply":"2021-06-10T13:40:45.01531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get full vocabulary of training data\nvocab = get_vocab(X_t)\n\n# reduce vocabulary \nred_vocab = reduce_vocab(3)\nprint('Length of train vocabulary:', len(vocab))\nprint('Length of reduced train vocabulary:', len(red_vocab))\n\n# transform reduced vocabulary to set\nvocab = set(red_vocab)\n\n# get preprocessed train and validation data\nX_tr_prepr = Cleaner().fit_transform(X_t)\nX_val_prepr = Cleaner().transform(X_v)\n\n# transform data based on reduced vocabulary\nX_tr_red = reduced_vocab_data(X_tr_prepr, vocab)\nX_val_red = reduced_vocab_data(X_val_prepr, vocab)\n\n# fit a tokenizer to train data\ntokenizer = create_tokenizer(X_tr_red)\n\n# calculate max data entry length\nlength = max_length(X_tr_red)\nprint('Max data entry length:', length)\n\n# define vocabulary size (required by Embedding layer)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary size:', vocab_size)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:41:13.783695Z","iopub.execute_input":"2021-06-10T13:41:13.784093Z","iopub.status.idle":"2021-06-10T13:41:19.014707Z","shell.execute_reply.started":"2021-06-10T13:41:13.784054Z","shell.execute_reply":"2021-06-10T13:41:19.01335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encode data\nX_tr = encode_data(tokenizer, X_tr_red, length)\nX_vl = encode_data(tokenizer, X_val_red, length)\nprint('Shape of training data:', X_tr.shape)\nprint('Shape of validation data:', X_vl.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:41:24.630377Z","iopub.execute_input":"2021-06-10T13:41:24.630754Z","iopub.status.idle":"2021-06-10T13:41:24.781492Z","shell.execute_reply.started":"2021-06-10T13:41:24.630724Z","shell.execute_reply":"2021-06-10T13:41:24.780167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(42)\n\n# define mode\nmodel = define_model(length, vocab_size)\ne_stop = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\nmodel.fit([X_tr, X_tr, X_tr], y_t, epochs=10, batch_size=32, \n          validation_data=([X_vl, X_vl, X_vl], y_v),\n          callbacks=[e_stop])\nmodel.evaluate([X_vl, X_vl, X_vl], y_v)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:01:24.743637Z","iopub.execute_input":"2021-06-10T13:01:24.744057Z","iopub.status.idle":"2021-06-10T13:01:49.827754Z","shell.execute_reply.started":"2021-06-10T13:01:24.744016Z","shell.execute_reply":"2021-06-10T13:01:49.826682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GloVe ","metadata":{}},{"cell_type":"code","source":"embedding_dict = {}\n\nwith open('../input/glovedata/glove.6B.200d.txt','r') as glove:\n    for line in glove:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors\n        \nglove.close()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:41:48.182498Z","iopub.execute_input":"2021-06-10T13:41:48.182893Z","iopub.status.idle":"2021-06-10T13:42:21.560835Z","shell.execute_reply.started":"2021-06-10T13:41:48.182862Z","shell.execute_reply":"2021-06-10T13:42:21.559726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, 200))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embedding_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:42:21.562886Z","iopub.execute_input":"2021-06-10T13:42:21.563681Z","iopub.status.idle":"2021-06-10T13:42:21.582528Z","shell.execute_reply.started":"2021-06-10T13:42:21.563638Z","shell.execute_reply":"2021-06-10T13:42:21.581252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed = Embedding(vocab_size, 200, weights=[embedding_matrix], trainable=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:42:21.584805Z","iopub.execute_input":"2021-06-10T13:42:21.585585Z","iopub.status.idle":"2021-06-10T13:42:21.592362Z","shell.execute_reply.started":"2021-06-10T13:42:21.58554Z","shell.execute_reply":"2021-06-10T13:42:21.591135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model setting were tried: \n- the number of filters: 3, 4, 5;\n- kernel size: [1, 2, 3, 4, 5], [2, 4, 6, 8, 10];\n- dropout rate: 0.1, 0.2, 0.3, 0.4, 0.5;\n- batch_size: 8, 16, 32, 64;\n- optimizer: Adam, Nadam, SGD, Momentum;\n- learning rate / momentum: [1e-3; 0.85], [1e-3; 0.9], [1e-3, 0.95], [1e-4; 0.85], [1e-4; 0.9], [1e-4, 0.95];\n\nBest configuration:\n- the number of filters: 5;\n- kernel size: [2, 4, 6, 8, 10];\n- dropout rate: 0.5;\n- batch_size: 32;\n- optimizer: Momentum;\n- learning rate / momentum: [1e-3; 0.85];","metadata":{}},{"cell_type":"code","source":"# define model\ndef glove_model(length, vocab_size, embed):\n    # channel 1\n    input1 = Input(shape=(length,))\n    embedding1 = embed(input1)\n    conv1 = Conv1D(filters=4, kernel_size=2, activation='elu')(embedding1)\n    norm1 = keras.layers.BatchNormalization()(conv1)\n    drop1 = Dropout(0.5)(norm1)\n    pool1 = MaxPooling1D(pool_size=2)(drop1)\n    flat1 = Flatten()(pool1)\n    # channel 2\n    input2 = Input(shape=(length,))\n    embedding2 = embed(input2)\n    conv2 = Conv1D(filters=4, kernel_size=3, activation='elu')(embedding2)\n    norm2 = keras.layers.BatchNormalization()(conv2)\n    drop2 = Dropout(0.5)(norm2)\n    pool2 = MaxPooling1D(pool_size=2)(drop2)\n    flat2 = Flatten()(pool2)\n    # channel 3\n    input3 = Input(shape=(length,))\n    embedding3 = embed(input3)\n    conv3 = Conv1D(filters=4, kernel_size=4, activation='elu')(embedding3)\n    norm3 = keras.layers.BatchNormalization()(conv3)\n    drop3 = Dropout(0.5)(norm3)\n    pool3 = MaxPooling1D(pool_size=2)(drop3)\n    flat3 = Flatten()(pool3)\n    # merge\n    merged = concatenate([flat1, flat2, flat3])\n    # interpretation\n    drop6 = Dropout(0.5)(merged)\n    dense = Dense(10, activation='elu')(drop6)\n    outputs = Dense(1, activation='sigmoid')(dense)\n    model = Model(inputs=[input1, input2, input3], outputs=outputs)\n    # compile\n    optimizer = keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:45:52.580629Z","iopub.execute_input":"2021-06-10T13:45:52.580984Z","iopub.status.idle":"2021-06-10T13:45:52.595842Z","shell.execute_reply.started":"2021-06-10T13:45:52.580954Z","shell.execute_reply":"2021-06-10T13:45:52.593064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(42)\n\n# define mode\nmodel = glove_model(length, vocab_size, embed)\n\ne_stop = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\nmodel.fit([X_tr, X_tr, X_tr], y_t, epochs=100, batch_size=32, \n          validation_data=([X_vl, X_vl, X_vl], y_v),\n          callbacks=[e_stop])\nmodel.evaluate([X_vl, X_vl, X_vl], y_v)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:46:18.29339Z","iopub.execute_input":"2021-06-10T13:46:18.29376Z","iopub.status.idle":"2021-06-10T13:47:41.380081Z","shell.execute_reply.started":"2021-06-10T13:46:18.293729Z","shell.execute_reply":"2021-06-10T13:47:41.378839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1cycle policy","metadata":{}},{"cell_type":"markdown","source":"1. Find maximum learning rate via log-LR / loss plot;\n2. Implement 1cycle scheduler and train model;","metadata":{}},{"cell_type":"code","source":"# define model\ndef glove_model(length, vocab_size, embed):\n    # channel 1\n    input1 = Input(shape=(length,))\n    embedding1 = embed(input1)\n    conv1 = Conv1D(filters=2, kernel_size=2, activation='elu')(embedding1)\n    drop1 = Dropout(0.5)(conv1)\n    pool1 = MaxPooling1D(pool_size=2)(drop1)\n    flat1 = Flatten()(pool1)\n    # channel 2\n    input2 = Input(shape=(length,))\n    embedding2 = embed(input2)\n    conv2 = Conv1D(filters=2, kernel_size=3, activation='elu')(embedding2)\n    drop2 = Dropout(0.5)(conv2)\n    pool2 = MaxPooling1D(pool_size=2)(drop2)\n    flat2 = Flatten()(pool2)\n    # channel 3\n    input3 = Input(shape=(length,))\n    embedding3 = embed(input3)\n    conv3 = Conv1D(filters=2, kernel_size=4, activation='elu')(embedding3)\n    drop3 = Dropout(0.5)(conv3)\n    pool3 = MaxPooling1D(pool_size=2)(drop3)\n    flat3 = Flatten()(pool3)\n    # merge\n    merged = concatenate([flat1, flat2, flat3])\n    # interpretation\n    drop6 = Dropout(0.5)(merged)\n    dense = Dense(10, activation='relu')(drop6)\n    outputs = Dense(1, activation='sigmoid')(dense)\n    model = Model(inputs=[input1, input2, input3], outputs=outputs)\n    # compile\n    optimizer = keras.optimizers.SGD(learning_rate=1e-3, momentum=0.85)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:54:04.600653Z","iopub.execute_input":"2021-06-10T13:54:04.601007Z","iopub.status.idle":"2021-06-10T13:54:04.613766Z","shell.execute_reply.started":"2021-06-10T13:54:04.600977Z","shell.execute_reply":"2021-06-10T13:54:04.61204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# exponential LR scheduler\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n    def on_batch_end(self, batch, logs):\n        self.rates.append(K.get_value(self.model.optimizer.lr))\n        self.losses.append(logs['loss'])\n        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n\n# get model losses based on exponentially changed learning rate       \ndef find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n    init_weights = model.get_weights()\n    iterations = len(X[0]) // batch_size * epochs\n    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n    init_lr = K.get_value(model.optimizer.lr)\n    K.set_value(model.optimizer.lr, min_rate)\n    exp_lr = ExponentialLearningRate(factor)\n    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n                        callbacks=[exp_lr])\n    K.set_value(model.optimizer.lr, init_lr)\n    model.set_weights(init_weights)\n    return exp_lr.rates, exp_lr.losses\n\n# log-LR / loss plot\ndef plot_lr_vs_loss(rates, losses):\n    plt.plot(rates, losses)\n    plt.gca().set_xscale('log')\n    plt.hlines(min(losses), min(rates), max(rates))\n    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.8])\n    plt.xlabel(\"Learning rate\")\n    plt.ylabel(\"Loss\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:54:05.023876Z","iopub.execute_input":"2021-06-10T13:54:05.024249Z","iopub.status.idle":"2021-06-10T13:54:05.038609Z","shell.execute_reply.started":"2021-06-10T13:54:05.02422Z","shell.execute_reply":"2021-06-10T13:54:05.037189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\ntf.keras.backend.clear_session()\n# define mode\ngl_model = glove_model(length, vocab_size, embed)\n# get rates and losses\nrates, losses = find_learning_rate(gl_model, [X_tr, X_tr, X_tr], y_t, epochs=1, batch_size=batch_size)\nplot_lr_vs_loss(rates, losses)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:54:06.645333Z","iopub.execute_input":"2021-06-10T13:54:06.645726Z","iopub.status.idle":"2021-06-10T13:54:09.108764Z","shell.execute_reply.started":"2021-06-10T13:54:06.645696Z","shell.execute_reply":"2021-06-10T13:54:09.107544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1cycle scheduler\nclass OneCycleScheduler(keras.callbacks.Callback):\n    \n    def __init__(self, iterations, max_rate, start_rate=None,\n                 last_iterations=None, last_rate=None):\n        self.iterations = iterations\n        self.max_momentum = 0.95\n        self.min_momentum = 0.85\n        self.max_rate = max_rate\n        self.start_rate = start_rate or max_rate / 10\n        self.last_iterations = last_iterations or iterations // 10 + 1\n        self.half_iteration = (iterations - self.last_iterations) // 2\n        self.last_rate = last_rate or self.start_rate / 1000\n        self.iteration = 0\n    \n    def _interpolate(self, iter1, iter2, rate1, rate2):\n        return ((rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1)\n    \n    def on_batch_begin(self, batch, logs):\n        if self.iteration < self.half_iteration:\n            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n            momentum = self._interpolate(0, self.half_iteration, self.max_momentum, self.min_momentum)\n        elif self.iteration < 2 * self.half_iteration:\n            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n                                     self.max_rate, self.start_rate)\n            momentum = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n                                     self.min_momentum, self.max_momentum)\n        else:\n            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n                                     self.start_rate, self.last_rate)\n            rate = max(rate, self.last_rate)\n            momentum = self._interpolate(2 * self.half_iteration, self.iterations,\n                                     self.max_momentum, self.min_momentum)\n            momentum = max(momentum, self.max_momentum)\n        self.iteration += 1\n        K.set_value(self.model.optimizer.lr, rate)\n        K.set_value(self.model.optimizer.momentum, momentum)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:54:09.110669Z","iopub.execute_input":"2021-06-10T13:54:09.111149Z","iopub.status.idle":"2021-06-10T13:54:09.125261Z","shell.execute_reply.started":"2021-06-10T13:54:09.111104Z","shell.execute_reply":"2021-06-10T13:54:09.124066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(42)\n\nn_epochs = 15\nbatch_size = 128\n# instantiate 1cycle callback\nonecycle = OneCycleScheduler(len(X_tr) // batch_size * n_epochs, max_rate=0.3)\n# define mode\ngl_model = glove_model(length, vocab_size, embed)\ngl_model.fit([X_tr, X_tr, X_tr], y_t, epochs=n_epochs, batch_size=batch_size, \n             validation_data=([X_vl, X_vl, X_vl], y_v), callbacks=[onecycle])\ngl_model.evaluate([X_vl, X_vl, X_vl], y_v)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:54:36.736823Z","iopub.execute_input":"2021-06-10T13:54:36.737227Z","iopub.status.idle":"2021-06-10T13:54:45.938735Z","shell.execute_reply.started":"2021-06-10T13:54:36.737197Z","shell.execute_reply":"2021-06-10T13:54:45.937709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tst = encode_data(tokenizer, X_test, length)\nprint('Shape of training data:', X_tst.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T00:14:05.408963Z","iopub.execute_input":"2021-06-10T00:14:05.409302Z","iopub.status.idle":"2021-06-10T00:14:05.489732Z","shell.execute_reply.started":"2021-06-10T00:14:05.409269Z","shell.execute_reply":"2021-06-10T00:14:05.488701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create submission file\ntest[\"target\"] = gl_model.predict([X_tst, X_tst, X_tst, X_tst, X_tst]).round().astype(int)\nsubmission = test[[\"id\", \"target\"]]\nsubmission.to_csv(\"sub_ss.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T00:14:17.645795Z","iopub.execute_input":"2021-06-10T00:14:17.646122Z","iopub.status.idle":"2021-06-10T00:14:18.248331Z","shell.execute_reply.started":"2021-06-10T00:14:17.64609Z","shell.execute_reply":"2021-06-10T00:14:18.247465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}