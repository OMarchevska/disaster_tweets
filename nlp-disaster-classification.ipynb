{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport string\nimport spacy\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nK = keras.backend\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Flatten, Dropout, Embedding\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-21T12:50:04.808248Z","iopub.execute_input":"2022-08-21T12:50:04.808561Z","iopub.status.idle":"2022-08-21T12:50:12.511147Z","shell.execute_reply.started":"2022-08-21T12:50:04.808485Z","shell.execute_reply":"2022-08-21T12:50:12.509535Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/glovedata/glove.6B.200d.txt\n/kaggle/input/glovedata/glove.6B.50d.txt\n/kaggle/input/glovedata/glove.6B.100d.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load and review data","metadata":{}},{"cell_type":"code","source":"# loading train and test set from kaggle public datasets\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\n\ntrain.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:50:56.871832Z","iopub.execute_input":"2022-08-21T12:50:56.872222Z","iopub.status.idle":"2022-08-21T12:50:56.949110Z","shell.execute_reply.started":"2022-08-21T12:50:56.872189Z","shell.execute_reply":"2022-08-21T12:50:56.948234Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"         id             keyword             location  \\\n2416   3475            derailed  Enterprise, Alabama   \n5717   8159            rescuers                  NaN   \n7325  10485            wildfire           Tucson, AZ   \n7028  10073             typhoon                  NaN   \n5067   7224  natural%20disaster                  NaN   \n5832   8332              rubble           California   \n2438   3501            derailed    Headed To The Top   \n971    1406          body%20bag      California, USA   \n6718   9620        thunderstorm  El Dorado, Arkansas   \n1809   2599               crash                  NaN   \n\n                                                   text  target  \n2416  Has #IdentityTheft Derailed Your #TaxReturn? \\...       0  \n5717  WomanÛªs GPS app guides rescuers to injured b...       1  \n7325  Does the #FingerRockFire make you wonder 'am I...       1  \n7028  Obama Declares Disaster for Typhoon-Devastated...       1  \n5067  Suncorp net profit rises to $1.13 billion in w...       1  \n5832  China's Stock Market Crash: Are There Gems In ...       0  \n2438        @OhYayyyYay the train derailed this morning       1  \n971   ?Ìü New Ladies Shoulder Tote #Handbag Faux Lea...       0  \n6718  NWS has Continued a Severe Thunderstorm Warnin...       1  \n1809  Crash helmet silvery floors karnal fat shoot s...       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2416</th>\n      <td>3475</td>\n      <td>derailed</td>\n      <td>Enterprise, Alabama</td>\n      <td>Has #IdentityTheft Derailed Your #TaxReturn? \\...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5717</th>\n      <td>8159</td>\n      <td>rescuers</td>\n      <td>NaN</td>\n      <td>WomanÛªs GPS app guides rescuers to injured b...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7325</th>\n      <td>10485</td>\n      <td>wildfire</td>\n      <td>Tucson, AZ</td>\n      <td>Does the #FingerRockFire make you wonder 'am I...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7028</th>\n      <td>10073</td>\n      <td>typhoon</td>\n      <td>NaN</td>\n      <td>Obama Declares Disaster for Typhoon-Devastated...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5067</th>\n      <td>7224</td>\n      <td>natural%20disaster</td>\n      <td>NaN</td>\n      <td>Suncorp net profit rises to $1.13 billion in w...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5832</th>\n      <td>8332</td>\n      <td>rubble</td>\n      <td>California</td>\n      <td>China's Stock Market Crash: Are There Gems In ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2438</th>\n      <td>3501</td>\n      <td>derailed</td>\n      <td>Headed To The Top</td>\n      <td>@OhYayyyYay the train derailed this morning</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>971</th>\n      <td>1406</td>\n      <td>body%20bag</td>\n      <td>California, USA</td>\n      <td>?Ìü New Ladies Shoulder Tote #Handbag Faux Lea...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6718</th>\n      <td>9620</td>\n      <td>thunderstorm</td>\n      <td>El Dorado, Arkansas</td>\n      <td>NWS has Continued a Severe Thunderstorm Warnin...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1809</th>\n      <td>2599</td>\n      <td>crash</td>\n      <td>NaN</td>\n      <td>Crash helmet silvery floors karnal fat shoot s...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# common abbreviations and contractions\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"€\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bro\" : \"brother\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"tv\": \"television\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w/\" : \"with\",\n    \"w/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\",\n    \"aren't\" : \"are not\",\n    \"can't\" : \"cannot\",\n    \"couldn't\" : \"could not\",\n    \"couldnt\" : \"could not\",\n    \"didn't\" : \"did not\",\n    \"doesn't\" : \"does not\",\n    \"doesnt\" : \"does not\",\n    \"don't\" : \"do not\",\n    \"hadn't\" : \"had not\",\n    \"hasn't\" : \"has not\",\n    \"haven't\" : \"have not\",\n    \"havent\" : \"have not\",\n    \"he'd\" : \"he would\",\n    \"he'll\" : \"he will\",\n    \"he's\" : \"he is\",\n    \"i'd\" : \"I would\",\n    \"i'd\" : \"I had\",\n    \"i'll\" : \"I will\",\n    \"i'm\" : \"I am\",\n    \"im\": 'I am',\n    \"isn't\" : \"is not\",\n    \"it's\" : \"it is\",\n    \"it'll\":\"it will\",\n    \"i've\" : \"I have\",\n    \"let's\" : \"let us\",\n    \"mightn't\" : \"might not\",\n    \"mustn't\" : \"must not\",\n    \"shan't\" : \"shall not\",\n    \"she'd\" : \"she would\",\n    \"she'll\" : \"she will\",\n    \"she's\" : \"she is\",\n    \"shouldn't\" : \"should not\",\n    \"shouldnt\" : \"should not\",\n    \"that's\" : \"that is\",\n    \"thats\" : \"that is\",\n    \"there's\" : \"there is\",\n    \"theres\" : \"there is\",\n    \"they'd\" : \"they would\",\n    \"they'll\" : \"they will\",\n    \"they're\" : \"they are\",\n    \"theyre\":  \"they are\",\n    \"they've\" : \"they have\",\n    \"we'd\" : \"we would\",\n    \"we're\" : \"we are\",\n    \"weren't\" : \"were not\",\n    \"we've\" : \"we have\",\n    \"what'll\" : \"what will\",\n    \"what're\" : \"what are\",\n    \"what's\" : \"what is\",\n    \"what've\" : \"what have\",\n    \"where's\" : \"where is\",\n    \"who'd\" : \"who would\",\n    \"who'll\" : \"who will\",\n    \"who're\" : \"who are\",\n    \"who's\" : \"who is\",\n    \"who've\" : \"who have\",\n    \"won't\" : \"will not\",\n    \"wouldn't\" : \"would not\",\n    \"you'd\" : \"you would\",\n    \"you'll\" : \"you will\",\n    \"you're\" : \"you are\",\n    \"you've\" : \"you have\",\n    \"'re\": \" are\",\n    \"wasn't\": \"was not\",\n    \"we'll\":\" will\",\n    \"didn't\": \"did not\"\n}","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:50:59.138295Z","iopub.execute_input":"2022-08-21T12:50:59.138704Z","iopub.status.idle":"2022-08-21T12:50:59.163424Z","shell.execute_reply.started":"2022-08-21T12:50:59.138642Z","shell.execute_reply":"2022-08-21T12:50:59.162562Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:51:00.722498Z","iopub.execute_input":"2022-08-21T12:51:00.722885Z","iopub.status.idle":"2022-08-21T12:51:00.739732Z","shell.execute_reply.started":"2022-08-21T12:51:00.722854Z","shell.execute_reply":"2022-08-21T12:51:00.738761Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7613 entries, 0 to 7612\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        7613 non-null   int64 \n 1   keyword   7552 non-null   object\n 2   location  5080 non-null   object\n 3   text      7613 non-null   object\n 4   target    7613 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 297.5+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"# drop duplicates\ntrain.drop_duplicates(subset=['text'], inplace=True)\n\n# get the features we want to analyze\nX_train = train['text'] \n# the labels\ny_train = train['target'] \n\nX_test = test['text'] ","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:51:02.569560Z","iopub.execute_input":"2022-08-21T12:51:02.569942Z","iopub.status.idle":"2022-08-21T12:51:02.581427Z","shell.execute_reply.started":"2022-08-21T12:51:02.569908Z","shell.execute_reply":"2022-08-21T12:51:02.580580Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Create our list of punctuation marks\npunctuations = string.punctuation\npunctuations += '...'\npunctuations += '....'\n\n# Create list of stopwords\nnlp = spacy.load('en')\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nparser = English()\n\n# Preliminary text cleaning\nclass Cleaner(TransformerMixin):\n    \n    def transform(self, X, **transform_params):\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\ndef clean_text(text):\n    # split data entry\n    text = text.split()\n    # omit hashtags\n    text = [word for word in text if not word.startswith('#')]\n    # join data entry back for tokenization and lemmatization\n    text = ' '.join(text)\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    mytokens = parser(text)\n    # Lemmatizing each token and converting each token into lowercase\n    mytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens]\n    # remove non-ascii\n    mytokens_enc = [word.encode(\"ascii\", \"ignore\") for word in mytokens]\n    mytokens = [word.decode() for word in mytokens_enc]\n    # Remove stop words\n    mytokens = [word for word in mytokens if word not in stop_words and word not in punctuations]\n    # replace numbers with 'number' token\n    mytokens = [word if not word.isnumeric() else 'number' for word in mytokens]\n    # replace abbreviations\n    mytokens = [abbreviations[word] if word in abbreviations.keys() else word for word in mytokens]\n    # remove mentions\n    mytokens = [word if not word.startswith('@') else 'user' for word in mytokens]\n    # removing URLs\n    mytokens = [word if not word.startswith('http') else 'url' for word in mytokens]\n    # remove other encodings\n    mytokens = [word for word in mytokens if not word.__contains__('\\x89')]\n    # remove out-of-vocabulary symbols\n    mytokens = [word for word in mytokens if not word.__contains__('amp')]\n    # strip any '/' character \n    mytokens = [word.replace('/', '') for word in mytokens]\n    # remove 2-letter words\n    mytokens = [word for word in mytokens if len(word) > 2]\n    # remove non-alphabetic words\n    mytokens = [word for word in mytokens if word.isalpha()]\n    return mytokens","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:51:14.791447Z","iopub.execute_input":"2022-08-21T12:51:14.791801Z","iopub.status.idle":"2022-08-21T12:51:15.894298Z","shell.execute_reply.started":"2022-08-21T12:51:14.791770Z","shell.execute_reply":"2022-08-21T12:51:15.893429Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Vocabulary reduction","metadata":{}},{"cell_type":"code","source":"# get train data vocabulary\ndef get_vocab(data):\n    # instantiate Counter()\n    vocab = Counter()\n    # preprocess data\n    cleaned = Cleaner().fit_transform(data)\n    # count number of each word occurrences in data \n    for line in cleaned:\n        vocab.update(line)\n    return vocab\n\n# keep tokens with a min occurrence\ndef reduce_vocab(min_occurane):\n    tokens = [k for k, c in vocab.items() if c >= min_occurane]\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:51:20.056416Z","iopub.execute_input":"2022-08-21T12:51:20.056782Z","iopub.status.idle":"2022-08-21T12:51:20.062412Z","shell.execute_reply.started":"2022-08-21T12:51:20.056750Z","shell.execute_reply":"2022-08-21T12:51:20.061533Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# final cleaning of data: omit words that are not present in reduced vocabulary\ndef reduced_vocab_data(data, vocab):\n    reduced_data = []\n    # walk through preprocessed data\n    for line in data:\n        # get only those words that are in reduced vocabulary\n        tokens = [word for word in line if word in vocab]\n        # update list with cleaned line of data\n        reduced_data.append(' '.join(tokens))\n    return reduced_data","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:51:21.731224Z","iopub.execute_input":"2022-08-21T12:51:21.731562Z","iopub.status.idle":"2022-08-21T12:51:21.736343Z","shell.execute_reply.started":"2022-08-21T12:51:21.731530Z","shell.execute_reply":"2022-08-21T12:51:21.735306Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Linear Classifiers","metadata":{}},{"cell_type":"code","source":"# split the dataset into training and validation sets\nX_t, X_v, y_t, y_v = train_test_split(X_train, y_train, test_size=0.15, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:51:23.921562Z","iopub.execute_input":"2022-08-21T12:51:23.921939Z","iopub.status.idle":"2022-08-21T12:51:23.928937Z","shell.execute_reply.started":"2022-08-21T12:51:23.921907Z","shell.execute_reply":"2022-08-21T12:51:23.928015Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# get a list of classifiers along with their names for evaluation on the dataset\nclassifiers = [('lr', LogisticRegression()),\n              ('svc', SVC()),\n              ('gbc', GradientBoostingClassifier(n_estimators=200))]\n\n# transform text into normalized bag-of-words\ntfidf_vector = TfidfVectorizer(tokenizer=clean_text)\n\n# Create a pipeline for each individual classifier by putting the text vectorizer in front of it\ndef get_pipe(classifiers):\n    pipes = []\n    for classifier in classifiers:\n        pipes.append((classifier[0], Pipeline([('vectorizer', tfidf_vector), classifier])))\n    return pipes\n\npipes = get_pipe(classifiers)\n\n# evaluate created pipelines\nfor pipe in pipes:\n    # model generation\n    pipe[1].fit(X_t, y_t)\n\n    # Predicting with a test dataset\n    predicted = pipe[1].predict(X_v)\n\n    # Model Accuracy\n    print(pipe[0] + ':', metrics.accuracy_score(y_v, predicted))","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:51:39.966258Z","iopub.execute_input":"2022-08-21T12:51:39.966615Z","iopub.status.idle":"2022-08-21T12:51:54.159014Z","shell.execute_reply.started":"2022-08-21T12:51:39.966580Z","shell.execute_reply":"2022-08-21T12:51:54.158130Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"lr: 0.7877442273534636\nsvc: 0.7921847246891652\ngbc: 0.7557726465364121\n","output_type":"stream"}]},{"cell_type":"code","source":"# get a stacking ensemble of models\ndef get_stacking(pipes):\n    # define the base models\n    level0 = list()\n    for pipe in pipes:\n        level0.append((str(pipe[0]), pipe[1]))\n    # define meta learner model\n    level1 = LogisticRegression()\n    # define the stacking ensemble\n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=3)\n    return model\n\n# Get a stacking ensemble, fit it to the data, and evaluate\nstacking = get_stacking(pipes)\nstacking.fit(X_t, y_t)\npredicted = stacking.predict(X_v)\n# Model Accuracy\nprint('stacking:', metrics.accuracy_score(y_v, predicted))","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:52:05.770566Z","iopub.execute_input":"2022-08-21T12:52:05.770965Z","iopub.status.idle":"2022-08-21T12:52:45.404301Z","shell.execute_reply.started":"2022-08-21T12:52:05.770931Z","shell.execute_reply":"2022-08-21T12:52:45.403472Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"stacking: 0.7948490230905861\n","output_type":"stream"}]},{"cell_type":"code","source":"# a submission file\ntest[\"target\"] = stacking.predict(X_test)\nsubmission = test[[\"id\", \"target\"]]\nsubmission.to_csv(\"sub_st.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:52:49.599129Z","iopub.execute_input":"2022-08-21T12:52:49.599451Z","iopub.status.idle":"2022-08-21T12:52:53.463786Z","shell.execute_reply.started":"2022-08-21T12:52:49.599421Z","shell.execute_reply":"2022-08-21T12:52:53.462734Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Multichannel Convolutional Neural Network","metadata":{}},{"cell_type":"markdown","source":"## Tokenization & Encoding","metadata":{}},{"cell_type":"code","source":"# fit a tokenizer\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\n# calculate the maximum document length\ndef max_length(lines):\n    return int(np.mean([len(line) for line in lines]))","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:53:02.316823Z","iopub.execute_input":"2022-08-21T12:53:02.317178Z","iopub.status.idle":"2022-08-21T12:53:02.322208Z","shell.execute_reply.started":"2022-08-21T12:53:02.317147Z","shell.execute_reply":"2022-08-21T12:53:02.321096Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# encode data\ndef encode_data(tokenizer, lines, size):\n    # integer encode\n    encoded = tokenizer.texts_to_sequences(lines)\n    # pad encoded sequences\n    padded = pad_sequences(encoded, maxlen=size, padding='post')\n    return padded","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:53:03.948280Z","iopub.execute_input":"2022-08-21T12:53:03.948625Z","iopub.status.idle":"2022-08-21T12:53:03.953563Z","shell.execute_reply.started":"2022-08-21T12:53:03.948592Z","shell.execute_reply":"2022-08-21T12:53:03.952277Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Define model","metadata":{}},{"cell_type":"code","source":"# define model\ndef define_model(length, vocab_size):\n    # channel 1\n    input1 = Input(shape=(length,))\n    embedding1 = Embedding(vocab_size, 25)(input1)\n    conv1 = Conv1D(filters=8, kernel_size=2, activation='elu')(embedding1)\n    drop1 = Dropout(0.4)(conv1)\n    pool1 = MaxPooling1D(pool_size=2)(drop1)\n    flat1 = Flatten()(pool1)\n    # channel 2\n    input2 = Input(shape=(length,))\n    embedding2 = Embedding(vocab_size, 25)(input2)\n    conv2 = Conv1D(filters=8, kernel_size=3, activation='elu')(embedding2)\n    drop2 = Dropout(0.4)(conv2)\n    pool2 = MaxPooling1D(pool_size=2)(drop2)\n    flat2 = Flatten()(pool2)\n    # channel 3\n    input3 = Input(shape=(length,))\n    embedding3 = Embedding(vocab_size, 25)(input3)\n    conv3 = Conv1D(filters=8, kernel_size=4, activation='elu')(embedding3)\n    drop3 = Dropout(0.4)(conv3)\n    pool3 = MaxPooling1D(pool_size=2)(drop3)\n    flat3 = Flatten()(pool3)\n    # merge\n    merged = concatenate([flat1, flat2, flat3])\n    # interpretation\n    drop4 = Dropout(0.1)(merged)\n    dense2 = Dense(10, activation='elu')(drop4)\n    outputs = Dense(1, activation='sigmoid')(dense2)\n    model = Model(inputs=[input1, input2, input3], outputs=outputs)\n    # compile\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:53:06.202405Z","iopub.execute_input":"2022-08-21T12:53:06.202767Z","iopub.status.idle":"2022-08-21T12:53:06.212067Z","shell.execute_reply.started":"2022-08-21T12:53:06.202734Z","shell.execute_reply":"2022-08-21T12:53:06.210860Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# get full vocabulary of training data\nvocab = get_vocab(X_t)\n\n# reduce vocabulary \nred_vocab = reduce_vocab(3)\nprint('Length of train vocabulary:', len(vocab))\nprint('Length of reduced train vocabulary:', len(red_vocab))\n\n# transform reduced vocabulary to set\nvocab = set(red_vocab)\n\n# get preprocessed train and validation data\nX_tr_prepr = Cleaner().fit_transform(X_t)\nX_val_prepr = Cleaner().transform(X_v)\n\n# transform data based on reduced vocabulary\nX_tr_red = reduced_vocab_data(X_tr_prepr, vocab)\nX_val_red = reduced_vocab_data(X_val_prepr, vocab)\n\n# fit a tokenizer to train data\ntokenizer = create_tokenizer(X_tr_red)\n\n# calculate max data entry length\nlength = max_length(X_tr_red)\nprint('Max data entry length:', length)\n\n# define vocabulary size (required by Embedding layer)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary size:', vocab_size)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:53:09.012499Z","iopub.execute_input":"2022-08-21T12:53:09.012867Z","iopub.status.idle":"2022-08-21T12:53:13.612032Z","shell.execute_reply.started":"2022-08-21T12:53:09.012833Z","shell.execute_reply":"2022-08-21T12:53:13.611121Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Length of train vocabulary: 10966\nLength of reduced train vocabulary: 3200\nMax data entry length: 44\nVocabulary size: 3201\n","output_type":"stream"}]},{"cell_type":"code","source":"# encode data\nX_tr = encode_data(tokenizer, X_tr_red, length)\nX_vl = encode_data(tokenizer, X_val_red, length)\nprint('Shape of training data:', X_tr.shape)\nprint('Shape of validation data:', X_vl.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:53:30.704999Z","iopub.execute_input":"2022-08-21T12:53:30.705340Z","iopub.status.idle":"2022-08-21T12:53:30.816561Z","shell.execute_reply.started":"2022-08-21T12:53:30.705311Z","shell.execute_reply":"2022-08-21T12:53:30.815701Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Shape of training data: (6377, 44)\nShape of validation data: (1126, 44)\n","output_type":"stream"}]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(42)\n\n# define, train, evaluate model\nmodel = define_model(length, vocab_size)\ne_stop = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\nmodel.fit([X_tr, X_tr, X_tr], y_t, epochs=10, batch_size=32, \n          validation_data=([X_vl, X_vl, X_vl], y_v),\n          callbacks=[e_stop])\nmodel.evaluate([X_vl, X_vl, X_vl], y_v)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T12:53:32.914066Z","iopub.execute_input":"2022-08-21T12:53:32.914401Z","iopub.status.idle":"2022-08-21T12:53:51.786903Z","shell.execute_reply.started":"2022-08-21T12:53:32.914371Z","shell.execute_reply":"2022-08-21T12:53:51.786012Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/10\n200/200 [==============================] - 7s 10ms/step - loss: 0.6536 - accuracy: 0.6073 - val_loss: 0.4934 - val_accuracy: 0.7647\nEpoch 2/10\n200/200 [==============================] - 1s 7ms/step - loss: 0.3892 - accuracy: 0.8349 - val_loss: 0.4611 - val_accuracy: 0.7869\nEpoch 3/10\n200/200 [==============================] - 1s 7ms/step - loss: 0.2860 - accuracy: 0.8805 - val_loss: 0.4957 - val_accuracy: 0.7824\nEpoch 4/10\n200/200 [==============================] - 1s 7ms/step - loss: 0.2142 - accuracy: 0.9237 - val_loss: 0.5464 - val_accuracy: 0.7664\nEpoch 5/10\n200/200 [==============================] - 1s 7ms/step - loss: 0.1737 - accuracy: 0.9338 - val_loss: 0.6172 - val_accuracy: 0.7709\nEpoch 6/10\n200/200 [==============================] - 1s 7ms/step - loss: 0.1404 - accuracy: 0.9500 - val_loss: 0.6831 - val_accuracy: 0.7602\nEpoch 7/10\n200/200 [==============================] - 2s 9ms/step - loss: 0.1273 - accuracy: 0.9541 - val_loss: 0.7123 - val_accuracy: 0.7673\n36/36 [==============================] - 0s 3ms/step - loss: 0.4611 - accuracy: 0.7869\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[0.4610981047153473, 0.7868561148643494]"},"metadata":{}}]},{"cell_type":"markdown","source":"## GloVe ","metadata":{}},{"cell_type":"code","source":"# build the embedding dictionary from .txt file \nembedding_dict = {}\n\nwith open('../input/glovedata/glove.6B.200d.txt','r') as glove:\n    for line in glove:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors\n        \nglove.close()","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:03:49.850572Z","iopub.execute_input":"2022-08-21T13:03:49.850992Z","iopub.status.idle":"2022-08-21T13:04:10.666915Z","shell.execute_reply.started":"2022-08-21T13:03:49.850956Z","shell.execute_reply":"2022-08-21T13:04:10.666004Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# build the embedding matrix to use it as weights in the model's Embedding layer \nembedding_matrix = np.zeros((vocab_size, 200))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embedding_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:04:10.669741Z","iopub.execute_input":"2022-08-21T13:04:10.670101Z","iopub.status.idle":"2022-08-21T13:04:10.685587Z","shell.execute_reply.started":"2022-08-21T13:04:10.670071Z","shell.execute_reply":"2022-08-21T13:04:10.684748Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"embed = Embedding(vocab_size, 200, weights=[embedding_matrix], trainable=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:04:10.687259Z","iopub.execute_input":"2022-08-21T13:04:10.687613Z","iopub.status.idle":"2022-08-21T13:04:10.693477Z","shell.execute_reply.started":"2022-08-21T13:04:10.687576Z","shell.execute_reply":"2022-08-21T13:04:10.692533Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Model setting were tried: \n- the number of filters: 3, 4, 5;\n- kernel size: [1, 2, 3, 4, 5], [2, 4, 6, 8, 10];\n- dropout rate: 0.1, 0.2, 0.3, 0.4, 0.5;\n- batch_size: 8, 16, 32, 64;\n- optimizer: Adam, Nadam, SGD, Momentum;\n- learning rate / momentum: [1e-3; 0.85], [1e-3; 0.9], [1e-3, 0.95], [1e-4; 0.85], [1e-4; 0.9], [1e-4, 0.95];\n\nBest configuration:\n- the number of filters: 5;\n- kernel size: [2, 4, 6, 8, 10];\n- dropout rate: 0.5;\n- batch_size: 32;\n- optimizer: Momentum;\n- learning rate / momentum: [1e-3; 0.85];","metadata":{}},{"cell_type":"code","source":"# define model\ndef glove_model(length, vocab_size, embed):\n    # channel 1\n    input1 = Input(shape=(length,))\n    embedding1 = embed(input1)\n    conv1 = Conv1D(filters=4, kernel_size=2, activation='elu')(embedding1)\n    norm1 = keras.layers.BatchNormalization()(conv1)\n    drop1 = Dropout(0.5)(norm1)\n    pool1 = MaxPooling1D(pool_size=2)(drop1)\n    flat1 = Flatten()(pool1)\n    # channel 2\n    input2 = Input(shape=(length,))\n    embedding2 = embed(input2)\n    conv2 = Conv1D(filters=4, kernel_size=3, activation='elu')(embedding2)\n    norm2 = keras.layers.BatchNormalization()(conv2)\n    drop2 = Dropout(0.5)(norm2)\n    pool2 = MaxPooling1D(pool_size=2)(drop2)\n    flat2 = Flatten()(pool2)\n    # channel 3\n    input3 = Input(shape=(length,))\n    embedding3 = embed(input3)\n    conv3 = Conv1D(filters=4, kernel_size=4, activation='elu')(embedding3)\n    norm3 = keras.layers.BatchNormalization()(conv3)\n    drop3 = Dropout(0.5)(norm3)\n    pool3 = MaxPooling1D(pool_size=2)(drop3)\n    flat3 = Flatten()(pool3)\n    # merge\n    merged = concatenate([flat1, flat2, flat3])\n    # interpretation\n    drop6 = Dropout(0.5)(merged)\n    dense = Dense(10, activation='elu')(drop6)\n    outputs = Dense(1, activation='sigmoid')(dense)\n    model = Model(inputs=[input1, input2, input3], outputs=outputs)\n    # compile\n    optimizer = keras.optimizers.SGD(learning_rate=1e-3, momentum=0.85)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:04:46.674809Z","iopub.execute_input":"2022-08-21T13:04:46.675144Z","iopub.status.idle":"2022-08-21T13:04:46.685948Z","shell.execute_reply.started":"2022-08-21T13:04:46.675114Z","shell.execute_reply":"2022-08-21T13:04:46.684788Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(42)\n\n# define mode\nmodel = glove_model(length, vocab_size, embed)\n\ne_stop = keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True)\nmodel.fit([X_tr, X_tr, X_tr], y_t, epochs=100, batch_size=32, \n          validation_data=([X_vl, X_vl, X_vl], y_v),\n          callbacks=[e_stop])\nmodel.evaluate([X_vl, X_vl, X_vl], y_v)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:06:23.035390Z","iopub.execute_input":"2022-08-21T13:06:23.035739Z","iopub.status.idle":"2022-08-21T13:08:11.290064Z","shell.execute_reply.started":"2022-08-21T13:06:23.035706Z","shell.execute_reply":"2022-08-21T13:08:11.289230Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch 1/100\n200/200 [==============================] - 3s 8ms/step - loss: 0.9155 - accuracy: 0.5538 - val_loss: 0.6179 - val_accuracy: 0.7078\nEpoch 2/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.6490 - accuracy: 0.6494 - val_loss: 0.5663 - val_accuracy: 0.7389\nEpoch 3/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.6055 - accuracy: 0.6835 - val_loss: 0.5342 - val_accuracy: 0.7540\nEpoch 4/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.5761 - accuracy: 0.7095 - val_loss: 0.5167 - val_accuracy: 0.7655\nEpoch 5/100\n200/200 [==============================] - 2s 8ms/step - loss: 0.5641 - accuracy: 0.7171 - val_loss: 0.5046 - val_accuracy: 0.7709\nEpoch 6/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.5441 - accuracy: 0.7345 - val_loss: 0.4990 - val_accuracy: 0.7744\nEpoch 7/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.5363 - accuracy: 0.7448 - val_loss: 0.4919 - val_accuracy: 0.7700\nEpoch 8/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.5256 - accuracy: 0.7404 - val_loss: 0.4883 - val_accuracy: 0.7771\nEpoch 9/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.5201 - accuracy: 0.7622 - val_loss: 0.4843 - val_accuracy: 0.7771\nEpoch 10/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.5181 - accuracy: 0.7515 - val_loss: 0.4804 - val_accuracy: 0.7762\nEpoch 11/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.5192 - accuracy: 0.7551 - val_loss: 0.4780 - val_accuracy: 0.7771\nEpoch 12/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.5077 - accuracy: 0.7606 - val_loss: 0.4769 - val_accuracy: 0.7753\nEpoch 13/100\n200/200 [==============================] - 2s 8ms/step - loss: 0.5092 - accuracy: 0.7578 - val_loss: 0.4775 - val_accuracy: 0.7753\nEpoch 14/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.5019 - accuracy: 0.7642 - val_loss: 0.4741 - val_accuracy: 0.7780\nEpoch 15/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.5125 - accuracy: 0.7672 - val_loss: 0.4728 - val_accuracy: 0.7771\nEpoch 16/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.5006 - accuracy: 0.7689 - val_loss: 0.4736 - val_accuracy: 0.7771\nEpoch 17/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.4950 - accuracy: 0.7762 - val_loss: 0.4726 - val_accuracy: 0.7744\nEpoch 18/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.4907 - accuracy: 0.7636 - val_loss: 0.4709 - val_accuracy: 0.7753\nEpoch 19/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.4795 - accuracy: 0.7767 - val_loss: 0.4697 - val_accuracy: 0.7798\nEpoch 20/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4968 - accuracy: 0.7673 - val_loss: 0.4680 - val_accuracy: 0.7815\nEpoch 21/100\n200/200 [==============================] - 2s 11ms/step - loss: 0.4671 - accuracy: 0.7871 - val_loss: 0.4682 - val_accuracy: 0.7762\nEpoch 22/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4894 - accuracy: 0.7785 - val_loss: 0.4675 - val_accuracy: 0.7753\nEpoch 23/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4755 - accuracy: 0.7837 - val_loss: 0.4661 - val_accuracy: 0.7815\nEpoch 24/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.4843 - accuracy: 0.7756 - val_loss: 0.4670 - val_accuracy: 0.7833\nEpoch 25/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4758 - accuracy: 0.7811 - val_loss: 0.4668 - val_accuracy: 0.7780\nEpoch 26/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4739 - accuracy: 0.7891 - val_loss: 0.4650 - val_accuracy: 0.7860\nEpoch 27/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.4806 - accuracy: 0.7775 - val_loss: 0.4656 - val_accuracy: 0.7762\nEpoch 28/100\n200/200 [==============================] - 2s 9ms/step - loss: 0.4838 - accuracy: 0.7772 - val_loss: 0.4663 - val_accuracy: 0.7824\nEpoch 29/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4793 - accuracy: 0.7794 - val_loss: 0.4650 - val_accuracy: 0.7833\nEpoch 30/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4808 - accuracy: 0.7739 - val_loss: 0.4655 - val_accuracy: 0.7780\nEpoch 31/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.4730 - accuracy: 0.7785 - val_loss: 0.4644 - val_accuracy: 0.7824\nEpoch 32/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.4656 - accuracy: 0.7905 - val_loss: 0.4628 - val_accuracy: 0.7806\nEpoch 33/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4742 - accuracy: 0.7836 - val_loss: 0.4641 - val_accuracy: 0.7851\nEpoch 34/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4800 - accuracy: 0.7760 - val_loss: 0.4646 - val_accuracy: 0.7824\nEpoch 35/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4627 - accuracy: 0.7866 - val_loss: 0.4633 - val_accuracy: 0.7833\nEpoch 36/100\n200/200 [==============================] - 2s 8ms/step - loss: 0.4734 - accuracy: 0.7824 - val_loss: 0.4626 - val_accuracy: 0.7789\nEpoch 37/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4678 - accuracy: 0.7843 - val_loss: 0.4646 - val_accuracy: 0.7842\nEpoch 38/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4647 - accuracy: 0.7880 - val_loss: 0.4629 - val_accuracy: 0.7806\nEpoch 39/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4700 - accuracy: 0.7922 - val_loss: 0.4626 - val_accuracy: 0.7798\nEpoch 40/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4572 - accuracy: 0.7946 - val_loss: 0.4623 - val_accuracy: 0.7851\nEpoch 41/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.4696 - accuracy: 0.7928 - val_loss: 0.4620 - val_accuracy: 0.7798\nEpoch 42/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.4573 - accuracy: 0.7933 - val_loss: 0.4622 - val_accuracy: 0.7789\nEpoch 43/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4566 - accuracy: 0.7949 - val_loss: 0.4659 - val_accuracy: 0.7806\nEpoch 44/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4703 - accuracy: 0.7924 - val_loss: 0.4629 - val_accuracy: 0.7806\nEpoch 45/100\n200/200 [==============================] - 2s 8ms/step - loss: 0.4520 - accuracy: 0.8022 - val_loss: 0.4632 - val_accuracy: 0.7735\nEpoch 46/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.4616 - accuracy: 0.7951 - val_loss: 0.4631 - val_accuracy: 0.7815\nEpoch 47/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4649 - accuracy: 0.7793 - val_loss: 0.4621 - val_accuracy: 0.7824\nEpoch 48/100\n200/200 [==============================] - 2s 8ms/step - loss: 0.4422 - accuracy: 0.8050 - val_loss: 0.4637 - val_accuracy: 0.7806\nEpoch 49/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4564 - accuracy: 0.7953 - val_loss: 0.4653 - val_accuracy: 0.7824\nEpoch 50/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4630 - accuracy: 0.7939 - val_loss: 0.4631 - val_accuracy: 0.7806\nEpoch 51/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4597 - accuracy: 0.7968 - val_loss: 0.4620 - val_accuracy: 0.7824\nEpoch 52/100\n200/200 [==============================] - 2s 8ms/step - loss: 0.4571 - accuracy: 0.7924 - val_loss: 0.4616 - val_accuracy: 0.7815\nEpoch 53/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4641 - accuracy: 0.7908 - val_loss: 0.4648 - val_accuracy: 0.7824\nEpoch 54/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4708 - accuracy: 0.7898 - val_loss: 0.4608 - val_accuracy: 0.7833\nEpoch 55/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4542 - accuracy: 0.7973 - val_loss: 0.4605 - val_accuracy: 0.7824\nEpoch 56/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4431 - accuracy: 0.7984 - val_loss: 0.4614 - val_accuracy: 0.7780\nEpoch 57/100\n200/200 [==============================] - 2s 10ms/step - loss: 0.4484 - accuracy: 0.7967 - val_loss: 0.4614 - val_accuracy: 0.7789\nEpoch 58/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4530 - accuracy: 0.7944 - val_loss: 0.4635 - val_accuracy: 0.7806\nEpoch 59/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4580 - accuracy: 0.7908 - val_loss: 0.4619 - val_accuracy: 0.7851\nEpoch 60/100\n200/200 [==============================] - 2s 8ms/step - loss: 0.4643 - accuracy: 0.7876 - val_loss: 0.4629 - val_accuracy: 0.7842\nEpoch 61/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4421 - accuracy: 0.7985 - val_loss: 0.4607 - val_accuracy: 0.7860\nEpoch 62/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4484 - accuracy: 0.8000 - val_loss: 0.4603 - val_accuracy: 0.7762\nEpoch 63/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4557 - accuracy: 0.7950 - val_loss: 0.4623 - val_accuracy: 0.7860\nEpoch 64/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4566 - accuracy: 0.7962 - val_loss: 0.4610 - val_accuracy: 0.7771\nEpoch 65/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4440 - accuracy: 0.8017 - val_loss: 0.4609 - val_accuracy: 0.7815\nEpoch 66/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4516 - accuracy: 0.7988 - val_loss: 0.4612 - val_accuracy: 0.7815\nEpoch 67/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4413 - accuracy: 0.8066 - val_loss: 0.4619 - val_accuracy: 0.7842\nEpoch 68/100\n200/200 [==============================] - 2s 8ms/step - loss: 0.4396 - accuracy: 0.8010 - val_loss: 0.4629 - val_accuracy: 0.7833\nEpoch 69/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4417 - accuracy: 0.8021 - val_loss: 0.4634 - val_accuracy: 0.7815\nEpoch 70/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4561 - accuracy: 0.7982 - val_loss: 0.4639 - val_accuracy: 0.7815\nEpoch 71/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.4190 - accuracy: 0.8119 - val_loss: 0.4631 - val_accuracy: 0.7798\nEpoch 72/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.4308 - accuracy: 0.8082 - val_loss: 0.4628 - val_accuracy: 0.7806\nEpoch 73/100\n200/200 [==============================] - 1s 6ms/step - loss: 0.4280 - accuracy: 0.8095 - val_loss: 0.4649 - val_accuracy: 0.7833\nEpoch 74/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4401 - accuracy: 0.8074 - val_loss: 0.4616 - val_accuracy: 0.7824\nEpoch 75/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4489 - accuracy: 0.7929 - val_loss: 0.4644 - val_accuracy: 0.7833\nEpoch 76/100\n200/200 [==============================] - 2s 8ms/step - loss: 0.4319 - accuracy: 0.8090 - val_loss: 0.4625 - val_accuracy: 0.7815\nEpoch 77/100\n200/200 [==============================] - 1s 7ms/step - loss: 0.4395 - accuracy: 0.8057 - val_loss: 0.4637 - val_accuracy: 0.7789\n36/36 [==============================] - 0s 3ms/step - loss: 0.4603 - accuracy: 0.7762\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"[0.46026816964149475, 0.7761989235877991]"},"metadata":{}}]},{"cell_type":"markdown","source":"### 1cycle policy","metadata":{}},{"cell_type":"markdown","source":"1. Find maximum learning rate via log-LR / loss plot;\n2. Implement 1cycle scheduler and train the model;","metadata":{}},{"cell_type":"code","source":"# exponential LR scheduler\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    \n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n        \n    def on_batch_end(self, batch, logs):\n        self.rates.append(K.get_value(self.model.optimizer.lr))\n        self.losses.append(logs['loss'])\n        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n\n# get model losses based on exponentially changed learning rate       \ndef find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n    init_weights = model.get_weights()\n    iterations = len(X[0]) // batch_size * epochs\n    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n    init_lr = K.get_value(model.optimizer.lr)\n    K.set_value(model.optimizer.lr, min_rate)\n    exp_lr = ExponentialLearningRate(factor)\n    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n                        callbacks=[exp_lr])\n    K.set_value(model.optimizer.lr, init_lr)\n    model.set_weights(init_weights)\n    return exp_lr.rates, exp_lr.losses\n\n# log-LR / loss plot\ndef plot_lr_vs_loss(rates, losses):\n    plt.plot(rates, losses)\n    plt.gca().set_xscale('log')\n    plt.hlines(min(losses), min(rates), max(rates))\n    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.8])\n    plt.xlabel(\"Learning rate\")\n    plt.ylabel(\"Loss\")","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:13:06.243256Z","iopub.execute_input":"2022-08-21T13:13:06.243582Z","iopub.status.idle":"2022-08-21T13:13:06.255410Z","shell.execute_reply.started":"2022-08-21T13:13:06.243553Z","shell.execute_reply":"2022-08-21T13:13:06.254397Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\ntf.keras.backend.clear_session()\n# define mode\ngl_model = glove_model(length, vocab_size, embed)\n# get rates and losses\nrates, losses = find_learning_rate(gl_model, [X_tr, X_tr, X_tr], y_t, epochs=1, batch_size=batch_size)\nplot_lr_vs_loss(rates, losses)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:14:18.769868Z","iopub.execute_input":"2022-08-21T13:14:18.770199Z","iopub.status.idle":"2022-08-21T13:14:22.130376Z","shell.execute_reply.started":"2022-08-21T13:14:18.770168Z","shell.execute_reply":"2022-08-21T13:14:22.129377Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"200/200 [==============================] - 2s 6ms/step - loss: nan - accuracy: 0.5071\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"# 1cycle scheduler\nclass OneCycleScheduler(keras.callbacks.Callback):\n    \n    def __init__(self, iterations, max_rate, start_rate=None,\n                 last_iterations=None, last_rate=None):\n        self.iterations = iterations\n        self.max_momentum = 0.95\n        self.min_momentum = 0.85\n        self.max_rate = max_rate\n        self.start_rate = start_rate or max_rate / 10\n        self.last_iterations = last_iterations or iterations // 10 + 1\n        self.half_iteration = (iterations - self.last_iterations) // 2\n        self.last_rate = last_rate or self.start_rate / 1000\n        self.iteration = 0\n    \n    def _interpolate(self, iter1, iter2, rate1, rate2):\n        return ((rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1)\n    \n    def on_batch_begin(self, batch, logs):\n        if self.iteration < self.half_iteration:\n            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n            momentum = self._interpolate(0, self.half_iteration, self.max_momentum, self.min_momentum)\n        elif self.iteration < 2 * self.half_iteration:\n            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n                                     self.max_rate, self.start_rate)\n            momentum = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n                                     self.min_momentum, self.max_momentum)\n        else:\n            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n                                     self.start_rate, self.last_rate)\n            rate = max(rate, self.last_rate)\n            momentum = self._interpolate(2 * self.half_iteration, self.iterations,\n                                     self.max_momentum, self.min_momentum)\n            momentum = max(momentum, self.max_momentum)\n        self.iteration += 1\n        K.set_value(self.model.optimizer.lr, rate)\n        K.set_value(self.model.optimizer.momentum, momentum)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:13:14.241718Z","iopub.execute_input":"2022-08-21T13:13:14.242062Z","iopub.status.idle":"2022-08-21T13:13:14.252409Z","shell.execute_reply.started":"2022-08-21T13:13:14.242034Z","shell.execute_reply":"2022-08-21T13:13:14.251184Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(42)\n\nn_epochs = 15\nbatch_size = 32\n# instantiate 1cycle callback\nonecycle = OneCycleScheduler(len(X_tr) // batch_size * n_epochs, max_rate=0.1)\n# define mode\ngl_model = glove_model(length, vocab_size, embed)\ngl_model.fit([X_tr, X_tr, X_tr], y_t, epochs=n_epochs, batch_size=batch_size, \n             validation_data=([X_vl, X_vl, X_vl], y_v), callbacks=[onecycle])\ngl_model.evaluate([X_vl, X_vl, X_vl], y_v)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:15:57.826294Z","iopub.execute_input":"2022-08-21T13:15:57.826633Z","iopub.status.idle":"2022-08-21T13:16:21.960444Z","shell.execute_reply.started":"2022-08-21T13:15:57.826601Z","shell.execute_reply":"2022-08-21T13:16:21.959670Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Epoch 1/15\n200/200 [==============================] - 3s 8ms/step - loss: 0.7484 - accuracy: 0.6118 - val_loss: 0.5455 - val_accuracy: 0.7309\nEpoch 2/15\n200/200 [==============================] - 2s 12ms/step - loss: 0.5510 - accuracy: 0.7359 - val_loss: 0.4844 - val_accuracy: 0.7709\nEpoch 3/15\n200/200 [==============================] - 1s 7ms/step - loss: 0.5376 - accuracy: 0.7382 - val_loss: 0.4924 - val_accuracy: 0.7647\nEpoch 4/15\n200/200 [==============================] - 1s 7ms/step - loss: 0.5178 - accuracy: 0.7535 - val_loss: 0.5054 - val_accuracy: 0.7691\nEpoch 5/15\n200/200 [==============================] - 1s 7ms/step - loss: 0.5123 - accuracy: 0.7599 - val_loss: 0.4852 - val_accuracy: 0.7682\nEpoch 6/15\n200/200 [==============================] - 1s 7ms/step - loss: 0.4938 - accuracy: 0.7824 - val_loss: 0.4736 - val_accuracy: 0.7798\nEpoch 7/15\n200/200 [==============================] - 1s 7ms/step - loss: 0.5030 - accuracy: 0.7693 - val_loss: 0.4789 - val_accuracy: 0.7771\nEpoch 8/15\n200/200 [==============================] - 1s 7ms/step - loss: 0.5017 - accuracy: 0.7658 - val_loss: 0.4943 - val_accuracy: 0.7611\nEpoch 9/15\n200/200 [==============================] - 2s 8ms/step - loss: 0.5049 - accuracy: 0.7742 - val_loss: 0.4717 - val_accuracy: 0.7780\nEpoch 10/15\n200/200 [==============================] - 1s 7ms/step - loss: 0.4844 - accuracy: 0.7816 - val_loss: 0.4865 - val_accuracy: 0.7735\nEpoch 11/15\n200/200 [==============================] - 1s 7ms/step - loss: 0.4869 - accuracy: 0.7763 - val_loss: 0.4714 - val_accuracy: 0.7762\nEpoch 12/15\n200/200 [==============================] - 1s 7ms/step - loss: 0.4673 - accuracy: 0.7930 - val_loss: 0.4741 - val_accuracy: 0.7744\nEpoch 13/15\n200/200 [==============================] - 1s 7ms/step - loss: 0.4716 - accuracy: 0.7873 - val_loss: 0.4812 - val_accuracy: 0.7762\nEpoch 14/15\n200/200 [==============================] - 1s 7ms/step - loss: 0.4681 - accuracy: 0.7895 - val_loss: 0.4702 - val_accuracy: 0.7762\nEpoch 15/15\n200/200 [==============================] - 1s 7ms/step - loss: 0.4639 - accuracy: 0.7885 - val_loss: 0.4730 - val_accuracy: 0.7851\n36/36 [==============================] - 0s 3ms/step - loss: 0.4730 - accuracy: 0.7851\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"[0.4729601740837097, 0.7850799560546875]"},"metadata":{}}]},{"cell_type":"code","source":"X_tst = encode_data(tokenizer, X_test, length)\nprint('Shape of training data:', X_tst.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:16:49.612479Z","iopub.execute_input":"2022-08-21T13:16:49.612870Z","iopub.status.idle":"2022-08-21T13:16:49.683815Z","shell.execute_reply.started":"2022-08-21T13:16:49.612837Z","shell.execute_reply":"2022-08-21T13:16:49.682865Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Shape of training data: (3263, 44)\n","output_type":"stream"}]},{"cell_type":"code","source":"# create submission file\ntest[\"target\"] = gl_model.predict([X_tst, X_tst, X_tst]).round().astype(int)\nsubmission = test[[\"id\", \"target\"]]\nsubmission.to_csv(\"sub_ss.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T13:20:04.841637Z","iopub.execute_input":"2022-08-21T13:20:04.841999Z","iopub.status.idle":"2022-08-21T13:20:05.332773Z","shell.execute_reply.started":"2022-08-21T13:20:04.841969Z","shell.execute_reply":"2022-08-21T13:20:05.331906Z"},"trusted":true},"execution_count":36,"outputs":[]}]}